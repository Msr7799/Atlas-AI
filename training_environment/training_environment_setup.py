#!/usr/bin/env python3
"""
ğŸ”¥ Ù†Ø¸Ø§Ù… Ø¥Ø¹Ø¯Ø§Ø¯ Ø¨ÙŠØ¦Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…
Ù†Ø¸Ø§Ù… Ø´Ø§Ù…Ù„ Ù„Ø¥Ø¹Ø¯Ø§Ø¯ ÙˆØªØ¯Ø±ÙŠØ¨ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Fine-Tuning
"""

import os
import sys
import json
import argparse
import logging
import subprocess
from pathlib import Path
from typing import Dict, List, Any, Optional
import time

# Ø¥Ø¹Ø¯Ø§Ø¯ Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ³Ø¬ÙŠÙ„
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('training_setup.log'),
        logging.StreamHandler(sys.stdout)
    ]
)

logger = logging.getLogger(__name__)

class TrainingEnvironmentSetup:
    """Ù†Ø¸Ø§Ù… Ø¥Ø¹Ø¯Ø§Ø¯ Ø¨ÙŠØ¦Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…ØªÙƒØ§Ù…Ù„"""
    
    def __init__(self, project_root: str):
        self.project_root = Path(project_root)
        self.assets_path = self.project_root / "assets" / "data" / "specialized_datasets"
        self.training_dir = self.project_root / "training_environment"
        self.python_env = self.training_dir / "venv"
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
        self.training_dir.mkdir(exist_ok=True)
        
    def check_system_requirements(self) -> Dict[str, bool]:
        """ÙØ­Øµ Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù…"""
        logger.info("ğŸ” ÙØ­Øµ Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù…...")
        
        requirements = {
            'python': self._check_python(),
            'pip': self._check_pip(),
            'git': self._check_git(),
            'cuda': self._check_cuda(),
            'data_exists': self._check_data_exists(),
            'disk_space': self._check_disk_space(),
        }
        
        # Ø·Ø¨Ø§Ø¹Ø© ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        logger.info("ğŸ“‹ Ù†ØªØ§Ø¦Ø¬ ÙØ­Øµ Ø§Ù„Ù†Ø¸Ø§Ù…:")
        for req, status in requirements.items():
            status_icon = "âœ…" if status else "âŒ"
            logger.info(f"  {status_icon} {req}: {'Ù…ØªÙˆÙØ±' if status else 'ØºÙŠØ± Ù…ØªÙˆÙØ±'}")
            
        return requirements
    
    def _check_python(self) -> bool:
        """ÙØ­Øµ ÙˆØ¬ÙˆØ¯ Python 3.8+"""
        try:
            result = subprocess.run(['python3', '--version'], capture_output=True, text=True)
            if result.returncode == 0:
                version = result.stdout.strip().split()[1]
                major, minor = map(int, version.split('.')[:2])
                return major >= 3 and minor >= 8
        except Exception as e:
            print(f"Error checking Python version: {e}")
        return False
    
    def _check_pip(self) -> bool:
        """ÙØ­Øµ ÙˆØ¬ÙˆØ¯ pip"""
        try:
            subprocess.run(['pip3', '--version'], capture_output=True, check=True)
            return True
        except:
            return False
    
    def _check_git(self) -> bool:
        """ÙØ­Øµ ÙˆØ¬ÙˆØ¯ Git"""
        try:
            subprocess.run(['git', '--version'], capture_output=True, check=True)
            return True
        except:
            return False
    
    def _check_cuda(self) -> bool:
        """ÙØ­Øµ ÙˆØ¬ÙˆØ¯ CUDA"""
        try:
            subprocess.run(['nvidia-smi'], capture_output=True, check=True)
            return True
        except:
            return False
    
    def _check_data_exists(self) -> bool:
        """ÙØ­Øµ ÙˆØ¬ÙˆØ¯ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨"""
        return self.assets_path.exists() and len(list(self.assets_path.glob('*.json'))) > 0
    
    def _check_disk_space(self) -> bool:
        """ÙØ­Øµ Ø§Ù„Ù…Ø³Ø§Ø­Ø© Ø§Ù„Ù…ØªØ§Ø­Ø© (5GB Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„)"""
        try:
            statvfs = os.statvfs(self.project_root)
            free_space = statvfs.f_frsize * statvfs.f_bavail
            return free_space > 5 * 1024 * 1024 * 1024  # 5GB
        except:
            return False
    
    def setup_python_environment(self) -> bool:
        """Ø¥Ø¹Ø¯Ø§Ø¯ Ø¨ÙŠØ¦Ø© Python Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©"""
        logger.info("ğŸ Ø¥Ø¹Ø¯Ø§Ø¯ Ø¨ÙŠØ¦Ø© Python...")
        
        try:
            # Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ¦Ø© Ø§ÙØªØ±Ø§Ø¶ÙŠØ©
            if not self.python_env.exists():
                logger.info("ğŸ“¦ Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ¦Ø© Ø§ÙØªØ±Ø§Ø¶ÙŠØ©...")
                subprocess.run([
                    'python3', '-m', 'venv', str(self.python_env)
                ], check=True)
            
            # ØªÙØ¹ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ¦Ø© ÙˆØªØ­Ø¯ÙŠØ« pip
            pip_cmd = str(self.python_env / 'bin' / 'pip')
            
            logger.info("â¬†ï¸ ØªØ­Ø¯ÙŠØ« pip...")
            subprocess.run([
                pip_cmd, 'install', '--upgrade', 'pip'
            ], check=True)
            
            # ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
            logger.info("ğŸ“š ØªØ«Ø¨ÙŠØª Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨...")
            required_packages = [
                'torch>=2.0.0',
                'transformers>=4.21.0',
                'datasets>=2.0.0',
                'tokenizers>=0.13.0',
                'accelerate>=0.20.0',
                'scikit-learn>=1.0.0',
                'pandas>=1.3.0',
                'numpy>=1.21.0',
                'tqdm>=4.64.0',
                'matplotlib>=3.5.0',
                'seaborn>=0.11.0',
                'tensorboard>=2.9.0',
                'psutil>=5.9.0',
            ]
            
            for package in required_packages:
                logger.info(f"ğŸ“¦ ØªØ«Ø¨ÙŠØª {package}...")
                subprocess.run([
                    pip_cmd, 'install', package
                ], check=True)
            
            logger.info("âœ… ØªÙ… Ø¥Ø¹Ø¯Ø§Ø¯ Ø¨ÙŠØ¦Ø© Python Ø¨Ù†Ø¬Ø§Ø­")
            return True
            
        except subprocess.CalledProcessError as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¥Ø¹Ø¯Ø§Ø¯ Ø¨ÙŠØ¦Ø© Python: {e}")
            return False
    
    def analyze_training_data(self) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨"""
        logger.info("ğŸ“Š ØªØ­Ù„ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨...")
        
        analysis = {
            'total_files': 0,
            'total_examples': 0,
            'total_size_mb': 0,
            'file_types': {},
            'sample_data': [],
            'estimated_training_time': 0,
        }
        
        try:
            json_files = list(self.assets_path.glob('*.json'))
            analysis['total_files'] = len(json_files)
            
            total_size = 0
            total_examples = 0
            
            for file_path in json_files[:10]:  # Ø¹ÙŠÙ†Ø© Ù…Ù† Ø£ÙˆÙ„ 10 Ù…Ù„ÙØ§Øª
                try:
                    file_size = file_path.stat().st_size
                    total_size += file_size
                    
                    with open(file_path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        
                    if isinstance(data, list):
                        total_examples += len(data)
                        if len(analysis['sample_data']) < 3:
                            analysis['sample_data'].extend(data[:2])
                    elif isinstance(data, dict):
                        total_examples += 1
                        if len(analysis['sample_data']) < 3:
                            analysis['sample_data'].append(data)
                            
                except Exception as e:
                    logger.warning(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© {file_path}: {e}")
            
            # ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ø­Ø¬Ù… Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ
            if json_files:
                avg_file_size = total_size / min(len(json_files), 10)
                estimated_total_size = avg_file_size * len(json_files)
                analysis['total_size_mb'] = estimated_total_size / (1024 * 1024)
                
                # ØªÙ‚Ø¯ÙŠØ± Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ù…Ø«Ù„Ø© Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ
                avg_examples_per_file = total_examples / min(len(json_files), 10)
                analysis['total_examples'] = int(avg_examples_per_file * len(json_files))
                
                # ØªÙ‚Ø¯ÙŠØ± ÙˆÙ‚Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ (Ø¯Ù‚Ø§Ø¦Ù‚)
                analysis['estimated_training_time'] = max(30, analysis['total_examples'] / 1000 * 5)
            
            logger.info(f"ğŸ“ˆ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:")
            logger.info(f"  ğŸ“ Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ù„ÙØ§Øª: {analysis['total_files']}")
            logger.info(f"  ğŸ“‹ Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ù…Ø«Ù„Ø©: {analysis['total_examples']:,}")
            logger.info(f"  ğŸ’¾ Ø§Ù„Ø­Ø¬Ù… Ø§Ù„Ù…Ù‚Ø¯Ø±: {analysis['total_size_mb']:.1f} MB")
            logger.info(f"  â±ï¸ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ù…Ù‚Ø¯Ø±: {analysis['estimated_training_time']:.0f} Ø¯Ù‚ÙŠÙ‚Ø©")
            
            return analysis
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")
            return analysis
    
    def create_training_script(self, config: Dict[str, Any]) -> str:
        """Ø¥Ù†Ø´Ø§Ø¡ Ø³ÙƒØ±ÙŠØ¨Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…Ø®ØµØµ"""
        logger.info("ğŸ“ Ø¥Ù†Ø´Ø§Ø¡ Ø³ÙƒØ±ÙŠÙ¾Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨...")
        
        script_content = f'''#!/usr/bin/env python3
"""
ğŸ”¥ Ø³ÙƒØ±ÙŠÙ¾Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…ØªÙ‚Ø¯Ù… Ù„Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ
ØªÙ… Ø¥Ù†Ø´Ø§Ø¤Ù‡ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ Ø¨ÙˆØ§Ø³Ø·Ø© Ù†Ø¸Ø§Ù… Fine-Tuning AI
"""

import json
import torch
import pandas as pd
from pathlib import Path
from transformers import (
    AutoTokenizer, AutoModelForCausalLM,
    TrainingArguments, Trainer,
    DataCollatorForLanguageModeling
)
from datasets import Dataset
import logging
import sys
from typing import Dict, List
import time

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ³Ø¬ÙŠÙ„
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('training_progress.log'),
        logging.StreamHandler(sys.stdout)
    ]
)

logger = logging.getLogger(__name__)

class AdvancedFineTuner:
    """Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.model_name = config.get('model_name', 'microsoft/DialoGPT-medium')
        self.max_length = config.get('max_length', 512)
        self.device = 'cuda' if torch.cuda.is_available() and config.get('use_cuda', True) else 'cpu'
        
        logger.info(f"ğŸš€ ØªÙ‡ÙŠØ¦Ø© Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¯Ø±ÙŠØ¨...")
        logger.info(f"  ğŸ¤– Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {{self.model_name}}")
        logger.info(f"  ğŸ’» Ø§Ù„Ø¬Ù‡Ø§Ø²: {{self.device}}")
        logger.info(f"  ğŸ“ Ø§Ù„Ø·ÙˆÙ„ Ø§Ù„Ø£Ù‚ØµÙ‰: {{self.max_length}}")
    
    def load_and_prepare_data(self, data_path: str) -> Dataset:
        """ØªØ­Ù…ÙŠÙ„ ÙˆØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        logger.info("ğŸ“Š ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨...")
        
        data_path = Path(data_path)
        all_texts = []
        
        # Ù‚Ø±Ø§Ø¡Ø© Ø¬Ù…ÙŠØ¹ Ù…Ù„ÙØ§Øª JSON
        json_files = list(data_path.glob('*.json'))
        logger.info(f"ğŸ“ Ø¹Ø«Ø± Ø¹Ù„Ù‰ {{len(json_files)}} Ù…Ù„Ù JSON")
        
        for file_path in json_files:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                if isinstance(data, list):
                    for item in data:
                        text = self._extract_text(item)
                        if text:
                            all_texts.append(text)
                elif isinstance(data, dict):
                    text = self._extract_text(data)
                    if text:
                        all_texts.append(text)
                        
            except Exception as e:
                logger.warning(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© {{file_path}}: {{e}}")
        
        logger.info(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ {{len(all_texts):,}} Ù†Øµ Ù„Ù„ØªØ¯Ø±ÙŠØ¨")
        
        # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Dataset
        dataset = Dataset.from_dict({{'text': all_texts}})
        return dataset
    
    def _extract_text(self, item: Dict) -> str:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ù…Ù† Ø¹Ù†ØµØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        if isinstance(item, str):
            return item
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø­Ù‚ÙˆÙ„ Ø§Ù„Ù†Øµ Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©
        text_fields = ['text', 'content', 'message', 'prompt', 'response', 'question', 'answer']
        
        for field in text_fields:
            if field in item and isinstance(item[field], str):
                return item[field]
        
        # Ø¥Ø°Ø§ Ù„Ù… ÙŠÙˆØ¬Ø¯ Ø­Ù‚Ù„ Ù†Øµ ÙˆØ§Ø¶Ø­ØŒ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù‚Ø§Ù…ÙˆØ³ Ø¥Ù„Ù‰ Ù†Øµ
        return str(item)
    
    def prepare_tokenizer_and_model(self):
        """ØªØ­Ø¶ÙŠØ± Ø§Ù„Ù…ÙØ±Ù…Ø² ÙˆØ§Ù„Ù†Ù…ÙˆØ°Ø¬"""
        logger.info("ğŸ”§ ØªØ­Ø¶ÙŠØ± Ø§Ù„Ù…ÙØ±Ù…Ø² ÙˆØ§Ù„Ù†Ù…ÙˆØ°Ø¬...")
        
        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙØ±Ù…Ø²
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        
        # Ø¥Ø¶Ø§ÙØ© Ø±Ù…Ø² Ø§Ù„Ø­Ø´Ùˆ Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ø§Ù‹
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.float16 if self.config.get('fp16', False) else torch.float32,
            device_map='auto' if self.device == 'cuda' else None
        )
        
        logger.info("âœ… ØªÙ… ØªØ­Ø¶ÙŠØ± Ø§Ù„Ù…ÙØ±Ù…Ø² ÙˆØ§Ù„Ù†Ù…ÙˆØ°Ø¬")
    
    def tokenize_dataset(self, dataset: Dataset) -> Dataset:
        """ØªØ±Ù…ÙŠØ² Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        logger.info("ğŸ”¤ ØªØ±Ù…ÙŠØ² Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")
        
        def tokenize_function(examples):
            return self.tokenizer(
                examples['text'],
                truncation=True,
                padding=True,
                max_length=self.max_length,
                return_tensors='pt'
            )
        
        tokenized_dataset = dataset.map(
            tokenize_function,
            batched=True,
            remove_columns=dataset.column_names
        )
        
        logger.info("âœ… ØªÙ… ØªØ±Ù…ÙŠØ² Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
        return tokenized_dataset
    
    def setup_training_arguments(self) -> TrainingArguments:
        """Ø¥Ø¹Ø¯Ø§Ø¯ Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨"""
        logger.info("âš™ï¸ Ø¥Ø¹Ø¯Ø§Ø¯ Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨...")
        
        return TrainingArguments(
            output_dir='./fine_tuned_model',
            num_train_epochs=self.config.get('epochs', 3),
            per_device_train_batch_size=self.config.get('batch_size', 4),
            gradient_accumulation_steps=self.config.get('gradient_accumulation_steps', 2),
            warmup_steps=self.config.get('warmup_steps', 500),
            learning_rate=self.config.get('learning_rate', 5e-5),
            fp16=self.config.get('fp16', False),
            logging_steps=self.config.get('logging_steps', 100),
            save_steps=self.config.get('save_steps', 1000),
            save_total_limit=2,
            prediction_loss_only=True,
            remove_unused_columns=False,
            dataloader_pin_memory=False,
        )
    
    def train_model(self, tokenized_dataset: Dataset):
        """ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""
        logger.info("ğŸ”¥ Ø¨Ø¯Ø¡ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬...")
        
        # Ø¥Ø¹Ø¯Ø§Ø¯ Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨
        training_args = self.setup_training_arguments()
        
        # Ø¥Ø¹Ø¯Ø§Ø¯ Data Collator
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=self.tokenizer,
            mlm=False
        )
        
        # Ø¥Ù†Ø´Ø§Ø¡ Trainer
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=tokenized_dataset,
            data_collator=data_collator,
            tokenizer=self.tokenizer,
        )
        
        # Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨
        start_time = time.time()
        trainer.train()
        end_time = time.time()
        
        training_time = end_time - start_time
        logger.info(f"âœ… Ø§Ù†ØªÙ‡Ù‰ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙÙŠ {{training_time/60:.1f}} Ø¯Ù‚ÙŠÙ‚Ø©")
        
        # Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
        trainer.save_model('./final_model')
        self.tokenizer.save_pretrained('./final_model')
        
        logger.info("ğŸ’¾ ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ")
        
        return trainer

def main():
    """Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
    logger.info("ğŸš€ Ø¨Ø¯Ø¡ Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…")
    
    # Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª
    config = {config}
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¯Ø±Ø¨
    trainer = AdvancedFineTuner(config)
    
    try:
        # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ù…ÙØ±Ù…Ø² ÙˆØ§Ù„Ù†Ù…ÙˆØ°Ø¬
        trainer.prepare_tokenizer_and_model()
        
        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        dataset = trainer.load_and_prepare_data("{str(self.assets_path)}")
        
        # ØªØ±Ù…ÙŠØ² Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        tokenized_dataset = trainer.tokenize_dataset(dataset)
        
        # ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        trained_model = trainer.train_model(tokenized_dataset)
        
        logger.info("ğŸ‰ ØªÙ… Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨Ù†Ø¬Ø§Ø­!")
        
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ¯Ø±ÙŠØ¨: {{e}}")
        sys.exit(1)

if __name__ == "__main__":
    main()
'''
        
        # ÙƒØªØ§Ø¨Ø© Ø§Ù„Ø³ÙƒØ±ÙŠÙ¾Øª
        script_path = self.training_dir / 'advanced_training.py'
        with open(script_path, 'w', encoding='utf-8') as f:
            f.write(script_content)
        
        # Ø¬Ø¹Ù„ Ø§Ù„Ø³ÙƒØ±ÙŠÙ¾Øª Ù‚Ø§Ø¨Ù„ Ù„Ù„ØªÙ†ÙÙŠØ°
        os.chmod(script_path, 0o755)
        
        logger.info(f"âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø³ÙƒØ±ÙŠÙ¾Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨: {script_path}")
        return str(script_path)
    
    def run_full_setup(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """ØªØ´ØºÙŠÙ„ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ÙƒØ§Ù…Ù„"""
        logger.info("ğŸ”¥ Ø¨Ø¯Ø¡ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ÙƒØ§Ù…Ù„ Ù„Ø¨ÙŠØ¦Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨")
        
        results = {
            'success': False,
            'requirements_check': {},
            'environment_setup': False,
            'data_analysis': {},
            'script_created': False,
            'script_path': '',
            'ready_for_training': False,
        }
        
        try:
            # 1. ÙØ­Øµ Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª
            results['requirements_check'] = self.check_system_requirements()
            
            # 2. Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¨ÙŠØ¦Ø©
            if all(results['requirements_check'].values()):
                results['environment_setup'] = self.setup_python_environment()
            else:
                logger.warning("âš ï¸ Ø¨Ø¹Ø¶ Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª ØºÙŠØ± Ù…ØªÙˆÙØ±Ø©")
            
            # 3. ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            results['data_analysis'] = self.analyze_training_data()
            
            # 4. Ø¥Ù†Ø´Ø§Ø¡ Ø³ÙƒØ±ÙŠÙ¾Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨
            if results['environment_setup']:
                results['script_path'] = self.create_training_script(config)
                results['script_created'] = True
            
            # 5. Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø¬Ø§Ù‡Ø²ÙŠØ© Ø§Ù„ÙƒØ§Ù…Ù„Ø©
            results['ready_for_training'] = (
                results['environment_setup'] and
                results['script_created'] and
                results['data_analysis']['total_examples'] > 0
            )
            
            results['success'] = True
            
            if results['ready_for_training']:
                logger.info("ğŸ‰ Ø§Ù„Ù†Ø¸Ø§Ù… Ø¬Ø§Ù‡Ø² ØªÙ…Ø§Ù…Ø§Ù‹ Ù„Ù„ØªØ¯Ø±ÙŠØ¨!")
            else:
                logger.warning("âš ï¸ Ø§Ù„Ù†Ø¸Ø§Ù… ØºÙŠØ± Ø¬Ø§Ù‡Ø² ØªÙ…Ø§Ù…Ø§Ù‹ Ù„Ù„ØªØ¯Ø±ÙŠØ¨")
                
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯: {e}")
            results['error'] = str(e)
        
        return results

def main():
    """Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
    parser = argparse.ArgumentParser(description='ğŸ”¥ Ù†Ø¸Ø§Ù… Ø¥Ø¹Ø¯Ø§Ø¯ Ø¨ÙŠØ¦Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…')
    parser.add_argument('--project-root', required=True, help='Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ')
    parser.add_argument('--config', help='Ù…Ù„Ù Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ (JSON)')
    
    args = parser.parse_args()
    
    # Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª
    default_config = {
        'model_name': 'microsoft/DialoGPT-medium',
        'epochs': 3,
        'batch_size': 4,
        'learning_rate': 5e-5,
        'max_length': 512,
        'warmup_steps': 500,
        'logging_steps': 100,
        'save_steps': 1000,
        'gradient_accumulation_steps': 2,
        'fp16': False,
        'use_cuda': True,
    }
    
    if args.config and Path(args.config).exists():
        with open(args.config, 'r') as f:
            user_config = json.load(f)
        default_config.update(user_config)
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø¸Ø§Ù… Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯
    setup_system = TrainingEnvironmentSetup(args.project_root)
    
    # ØªØ´ØºÙŠÙ„ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ÙƒØ§Ù…Ù„
    results = setup_system.run_full_setup(default_config)
    
    # Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    print("\\n" + "="*60)
    print("ğŸ”¥ Ù†ØªØ§Ø¦Ø¬ Ø¥Ø¹Ø¯Ø§Ø¯ Ø¨ÙŠØ¦Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨")
    print("="*60)
    
    if results['success']:
        print("âœ… ØªÙ… Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯ Ø¨Ù†Ø¬Ø§Ø­")
        if results['ready_for_training']:
            print("ğŸš€ Ø§Ù„Ù†Ø¸Ø§Ù… Ø¬Ø§Ù‡Ø² Ù„Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨!")
            print(f"ğŸ“ Ø³ÙƒØ±ÙŠÙ¾Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨: {results['script_path']}")
            print(f"ğŸ“Š Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ù…Ø«Ù„Ø©: {results['data_analysis']['total_examples']:,}")
            print(f"â±ï¸ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ù…Ù‚Ø¯Ø±: {results['data_analysis']['estimated_training_time']:.0f} Ø¯Ù‚ÙŠÙ‚Ø©")
        else:
            print("âš ï¸ Ø¨Ø¹Ø¶ Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª ØºÙŠØ± Ù…ÙƒØªÙ…Ù„Ø©")
    else:
        print("âŒ ÙØ´Ù„ ÙÙŠ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯")
        if 'error' in results:
            print(f"Ø§Ù„Ø®Ø·Ø£: {results['error']}")

if __name__ == "__main__":
    main()
