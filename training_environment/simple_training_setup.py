#!/usr/bin/env python3
"""
ğŸ”¥ Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…Ø¨Ø³Ø· ÙˆØ§Ù„Ù…Ø­Ø³Ù†
Ù†Ø¸Ø§Ù… Ù…Ø¨Ø³Ø· Ù„Ø¥Ø¹Ø¯Ø§Ø¯ ÙˆØªØ¯Ø±ÙŠØ¨ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ
"""

import json
import subprocess
import sys
import os
from pathlib import Path
import logging

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ³Ø¬ÙŠÙ„
logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
logger = logging.getLogger(__name__)

def check_and_install_requirements():
    """ÙØ­Øµ ÙˆØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©"""
    logger.info("ğŸ” ÙØ­Øµ Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª...")
    
    required_packages = [
        'torch',
        'transformers', 
        'datasets',
        'numpy',
        'pandas',
        'tqdm'
    ]
    
    missing_packages = []
    
    for package in required_packages:
        try:
            __import__(package)
            logger.info(f"âœ… {package} Ù…ØªÙˆÙØ±")
        except ImportError:
            missing_packages.append(package)
            logger.warning(f"âŒ {package} ØºÙŠØ± Ù…ØªÙˆÙØ±")
    
    if missing_packages:
        logger.info(f"ğŸ“¦ ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©: {', '.join(missing_packages)}")
        
        try:
            # ØªØ«Ø¨ÙŠØª PyTorch CPU version
            if 'torch' in missing_packages:
                subprocess.check_call([
                    sys.executable, '-m', 'pip', 'install', 
                    'torch', 'torchvision', 'torchaudio', '--index-url', 
                    'https://download.pytorch.org/whl/cpu'
                ])
                missing_packages.remove('torch')
            
            # ØªØ«Ø¨ÙŠØª Ø¨Ø§Ù‚ÙŠ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª
            if missing_packages:
                subprocess.check_call([
                    sys.executable, '-m', 'pip', 'install'
                ] + missing_packages)
                
            logger.info("âœ… ØªÙ… ØªØ«Ø¨ÙŠØª Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø¨Ù†Ø¬Ø§Ø­")
            return True
            
        except subprocess.CalledProcessError as e:
            logger.error(f"âŒ ÙØ´Ù„ ÙÙŠ ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª: {e}")
            return False
    
    logger.info("âœ… Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ù…ØªÙˆÙØ±Ø©")
    return True

def analyze_training_data(data_path):
    """ØªØ­Ù„ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨"""
    logger.info("ğŸ“Š ØªØ­Ù„ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨...")
    
    data_path = Path(data_path)
    
    if not data_path.exists():
        logger.error(f"âŒ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯: {data_path}")
        return None
    
    json_files = list(data_path.glob('*.json'))
    
    if not json_files:
        logger.error("âŒ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ù„ÙØ§Øª JSON ÙÙŠ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
        return None
    
    total_examples = 0
    total_size = 0
    
    for file_path in json_files:
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            if isinstance(data, list):
                total_examples += len(data)
            else:
                total_examples += 1
                
            total_size += file_path.stat().st_size
            
        except Exception as e:
            logger.warning(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© {file_path}: {e}")
    
    logger.info(f"ğŸ“ Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ù„ÙØ§Øª: {len(json_files)}")
    logger.info(f"ğŸ“‹ Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ù…Ø«Ù„Ø©: {total_examples:,}")
    logger.info(f"ğŸ’¾ Ø§Ù„Ø­Ø¬Ù… Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ: {total_size / (1024*1024):.1f} MB")
    
    return {
        'files_count': len(json_files),
        'examples_count': total_examples,
        'total_size_mb': total_size / (1024*1024)
    }

def create_simple_training_script(data_path, config):
    """Ø¥Ù†Ø´Ø§Ø¡ Ø³ÙƒØ±ÙŠÙ¾Øª ØªØ¯Ø±ÙŠØ¨ Ù…Ø¨Ø³Ø·"""
    logger.info("ğŸ“ Ø¥Ù†Ø´Ø§Ø¡ Ø³ÙƒØ±ÙŠÙ¾Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨...")
    
    script_content = f'''#!/usr/bin/env python3
"""
ğŸ”¥ Ø³ÙƒØ±ÙŠÙ¾Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…Ø¨Ø³Ø·
"""

import json
import torch
from pathlib import Path
from transformers import (
    AutoTokenizer, AutoModelForCausalLM,
    TrainingArguments, Trainer,
    DataCollatorForLanguageModeling
)
from datasets import Dataset
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def load_data():
    """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
    logger.info("ğŸ“Š ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")
    
    data_path = Path("{data_path}")
    all_texts = []
    
    for json_file in data_path.glob('*.json'):
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            if isinstance(data, list):
                for item in data:
                    if isinstance(item, dict):
                        # Ø¯Ù…Ø¬ prompt Ùˆ response
                        if 'prompt' in item and 'response' in item:
                            text = f"Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: {{item['prompt']}}\\nØ§Ù„Ù…Ø³Ø§Ø¹Ø¯: {{item['response']}}"
                        elif 'text' in item:
                            text = item['text']
                        else:
                            text = str(item)
                    else:
                        text = str(item)
                    all_texts.append(text)
            else:
                all_texts.append(str(data))
                
        except Exception as e:
            logger.warning(f"Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© {{json_file}}: {{e}}")
    
    logger.info(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ {{len(all_texts)}} Ù†Øµ")
    return all_texts

def main():
    """Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
    logger.info("ğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨...")
    
    try:
        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        texts = load_data()
        
        if not texts:
            logger.error("âŒ Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„ØªØ¯Ø±ÙŠØ¨")
            return
        
        # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ù„Ù…ÙØ±Ù…Ø²
        model_name = "{config.get('model_name', 'microsoft/DialoGPT-small')}"
        logger.info(f"ğŸ¤– ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {{model_name}}")
        
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForCausalLM.from_pretrained(model_name)
        
        # Ø¥Ø¶Ø§ÙØ© pad_token Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ø§Ù‹
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        dataset = Dataset.from_dict({{'text': texts}})
        
        def tokenize_function(examples):
            return tokenizer(
                examples['text'],
                truncation=True,
                padding=True,
                max_length={config.get('max_length', 256)},
                return_tensors='pt'
            )
        
        tokenized_dataset = dataset.map(tokenize_function, batched=True)
        
        # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨
        training_args = TrainingArguments(
            output_dir='./results',
            num_train_epochs={config.get('epochs', 1)},
            per_device_train_batch_size={config.get('batch_size', 2)},
            warmup_steps={config.get('warmup_steps', 50)},
            learning_rate={config.get('learning_rate', 3e-5)},
            logging_steps={config.get('logging_steps', 10)},
            save_steps={config.get('save_steps', 100)},
            save_total_limit=1,
            remove_unused_columns=False,
        )
        
        # Ø¥Ø¹Ø¯Ø§Ø¯ Data Collator
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=tokenizer,
            mlm=False
        )
        
        # Ø¥Ù†Ø´Ø§Ø¡ Trainer
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=tokenized_dataset,
            data_collator=data_collator,
            tokenizer=tokenizer,
        )
        
        # Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨
        logger.info("ğŸ”¥ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨...")
        trainer.train()
        
        # Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        trainer.save_model('./fine_tuned_model')
        tokenizer.save_pretrained('./fine_tuned_model')
        
        logger.info("ğŸ‰ ØªÙ… Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨Ù†Ø¬Ø§Ø­!")
        logger.info("ğŸ’¾ ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ: ./fine_tuned_model")
        
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ¯Ø±ÙŠØ¨: {{e}}")
        return False
    
    return True

if __name__ == "__main__":
    main()
'''
    
    script_path = Path('simple_training.py')
    with open(script_path, 'w', encoding='utf-8') as f:
        f.write(script_content)
    
    os.chmod(script_path, 0o755)
    
    logger.info(f"âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø³ÙƒØ±ÙŠÙ¾Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨: {script_path}")
    return str(script_path)

def main():
    """Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
    print("ğŸ”¥ Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…Ø¨Ø³Ø· ÙˆØ§Ù„Ù…Ø­Ø³Ù†")
    print("="*50)
    
    # Ù…Ø³Ø§Ø± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    data_path = Path("assets/data/specialized_datasets")
    
    # 1. ÙØ­Øµ ÙˆØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª
    if not check_and_install_requirements():
        print("âŒ ÙØ´Ù„ ÙÙŠ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª")
        return
    
    # 2. ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    data_analysis = analyze_training_data(data_path)
    if not data_analysis:
        print("âŒ ÙØ´Ù„ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
        return
    
    # 3. Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª
    config_path = Path("training_config.json")
    if config_path.exists():
        with open(config_path, 'r') as f:
            config = json.load(f)
    else:
        config = {
            'model_name': 'microsoft/DialoGPT-small',
            'epochs': 1,
            'batch_size': 2,
            'learning_rate': 3e-5,
            'max_length': 256,
            'warmup_steps': 50,
            'logging_steps': 10,
            'save_steps': 100
        }
    
    # 4. Ø¥Ù†Ø´Ø§Ø¡ Ø³ÙƒØ±ÙŠÙ¾Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨
    script_path = create_simple_training_script(data_path, config)
    
    # 5. Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    print("\\nâœ… Ø§Ù„Ù†Ø¸Ø§Ù… Ø¬Ø§Ù‡Ø² Ù„Ù„ØªØ¯Ø±ÙŠØ¨!")
    print(f"ğŸ“ Ø³ÙƒØ±ÙŠÙ¾Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨: {script_path}")
    print(f"ğŸ“Š Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ù…Ø«Ù„Ø©: {data_analysis['examples_count']:,}")
    print(f"ğŸ’¾ Ø­Ø¬Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {data_analysis['total_size_mb']:.1f} MB")
    print("\\nğŸš€ Ù„Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ØŒ Ø´ØºÙ„ Ø§Ù„Ø£Ù…Ø±:")
    print(f"python3 {script_path}")

if __name__ == "__main__":
    main()
