import 'dart:async';
import 'dart:convert';
import 'dart:io';
import 'package:flutter/services.dart';
import 'package:path_provider/path_provider.dart';

class ModelTrainingService {
  static final ModelTrainingService _instance =
      ModelTrainingService._internal();
  factory ModelTrainingService() => _instance;
  ModelTrainingService._internal();

  // Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨
  String? _trainingDataPath;
  final bool _isTraining = false;
  final double _trainingProgress = 0.0;
  final List<Map<String, dynamic>> _trainingLogs = [];

  // Ø¥Ø¹Ø¯Ø§Ø¯ Ø¨ÙŠØ¦Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨
  Future<bool> initializeTrainingEnvironment() async {
    try {
      print('[MODEL_TRAINING] ğŸš€ Ø¨Ø¯Ø¡ Ø¥Ø¹Ø¯Ø§Ø¯ Ø¨ÙŠØ¦Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨...');

      // Ø¥Ø¹Ø¯Ø§Ø¯ Ù…Ø¬Ù„Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨
      final appDir = await getApplicationDocumentsDirectory();
      final trainingDir = Directory('${appDir.path}/fine_tuning_training');
      if (!await trainingDir.exists()) {
        await trainingDir.create(recursive: true);
      }
      _trainingDataPath = trainingDir.path;

      // ØªØ­Ù…ÙŠÙ„ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨
      await _loadTrainingConfig();

      // Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
      await _prepareTrainingFiles();

      print('[MODEL_TRAINING] âœ… ØªÙ… Ø¥Ø¹Ø¯Ø§Ø¯ Ø¨ÙŠØ¦Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨Ù†Ø¬Ø§Ø­');
      return true;
    } catch (e) {
      print('[MODEL_TRAINING] âŒ Ø®Ø·Ø£ ÙÙŠ Ø¥Ø¹Ø¯Ø§Ø¯ Ø¨ÙŠØ¦Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨: $e');
      return false;
    }
  }

  // ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù…Ù† assets
  Future<void> _prepareTrainingFiles() async {
    try {
      print('[MODEL_TRAINING] ğŸ“Š ØªØ­Ø¶ÙŠØ± Ù…Ù„ÙØ§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨...');

      // Ù‚Ø§Ø¦Ù…Ø© Ù…Ù„ÙØ§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
      final datasetFiles = [
        'assets/data/specialized_datasets/fine_Tuning.json',
        'assets/data/specialized_datasets/finetuning_examples.parquet',
        'assets/data/specialized_datasets/css_code_dataset.parquet',
      ];

      // Ù†Ø³Ø® Ø§Ù„Ù…Ù„ÙØ§Øª Ø¥Ù„Ù‰ Ù…Ø¬Ù„Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨
      for (final assetPath in datasetFiles) {
        try {
          final data = await rootBundle.load(assetPath);
          final fileName = assetPath.split('/').last;
          final targetFile = File('$_trainingDataPath/$fileName');

          await targetFile.writeAsBytes(data.buffer.asUint8List());
          print('[MODEL_TRAINING] âœ… ØªÙ… Ù†Ø³Ø®: $fileName');
        } catch (e) {
          print('[MODEL_TRAINING] âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰: $assetPath');
        }
      }

      // Ø¥Ù†Ø´Ø§Ø¡ Ø³ÙƒØ±ÙŠØ¨Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Python
      await _createTrainingScript();
    } catch (e) {
      print('[MODEL_TRAINING] âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ø¶ÙŠØ± Ø§Ù„Ù…Ù„ÙØ§Øª: $e');
    }
  }

  // Ø¥Ù†Ø´Ø§Ø¡ Ø³ÙƒØ±ÙŠØ¨Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Python
  Future<void> _createTrainingScript() async {
    final scriptContent = '''#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…ØªÙ‚Ø¯Ù… Ù„Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù„ØºÙˆÙŠØ©
Fine-Tuning System for Language Models
"""

import os
import json
import pandas as pd
import numpy as np
import torch
import transformers
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from datasets import Dataset, load_dataset
import wandb
from datetime import datetime
import argparse
import logging

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ³Ø¬ÙŠÙ„
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('training.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class FineTuningTrainer:
    def __init__(self, config_path="training_config.json"):
        """ØªÙ‡ÙŠØ¦Ø© Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¯Ø±ÙŠØ¨"""
        self.config = self.load_config(config_path)
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        logger.info(f"ğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¹Ù„Ù‰ {self.device}")
        
    def load_config(self, config_path):
        """ØªØ­Ù…ÙŠÙ„ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨"""
        default_config = {
            "model_name": "microsoft/DialoGPT-medium",
            "dataset_path": "./fine_Tuning.json",
            "output_dir": "./fine_tuned_model",
            "num_train_epochs": 3,
            "per_device_train_batch_size": 4,
            "per_device_eval_batch_size": 4,
            "learning_rate": 5e-5,
            "weight_decay": 0.01,
            "logging_steps": 10,
            "save_steps": 500,
            "eval_steps": 500,
            "warmup_steps": 100,
            "max_length": 512
        }
        
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
            return {**default_config, **config}
        except FileNotFoundError:
            logger.warning("âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ù„Ù Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§ØªØŒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©")
            return default_config
    
    def prepare_dataset(self):
        """ØªØ­Ø¶ÙŠØ± Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨"""
        logger.info("ğŸ“Š ØªØ­Ø¶ÙŠØ± Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨...")
        
        try:
            # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† JSON
            with open(self.config["dataset_path"], 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†ØµÙˆØµ Ù…Ù† Ø®Ù„Ø§ÙŠØ§ Ø§Ù„ÙƒÙˆØ¯
            texts = []
            if 'cells' in data:
                for cell in data['cells']:
                    if cell.get('cell_type') == 'code' and cell.get('source'):
                        source = cell['source']
                        if isinstance(source, list):
                            text = ''.join(source)
                        else:
                            text = str(source)
                        
                        if len(text.strip()) > 10:  # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù‚ØµÙŠØ±Ø© Ø¬Ø¯Ø§Ù‹
                            texts.append(text.strip())
            
            logger.info(f"ğŸ“ˆ ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ {len(texts)} Ø¹ÙŠÙ†Ø© ØªØ¯Ø±ÙŠØ¨")
            
            # Ø¥Ù†Ø´Ø§Ø¡ Dataset
            dataset = Dataset.from_dict({"text": texts})
            
            # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            split_dataset = dataset.train_test_split(test_size=0.1)
            
            return split_dataset["train"], split_dataset["test"]
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")
            raise
    
    def tokenize_function(self, examples):
        """ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ tokens"""
        return self.tokenizer(
            examples["text"],
            truncation=True,
            padding=True,
            max_length=self.config["max_length"]
        )
    
    def train_model(self):
        """Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¯Ø±ÙŠØ¨"""
        try:
            logger.info("ğŸ¤– ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ù„Ù…Ø­Ù„Ù„ Ø§Ù„Ù„ØºÙˆÙŠ...")
            
            # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ù„ØªÙˆÙƒÙ†Ø§ÙŠØ±
            self.tokenizer = AutoTokenizer.from_pretrained(self.config["model_name"])
            
            # Ø¥Ø¶Ø§ÙØ© pad token Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ø§Ù‹
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            model = AutoModelForCausalLM.from_pretrained(
                self.config["model_name"],
                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
            )
            
            # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            train_dataset, eval_dataset = self.prepare_dataset()
            
            # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªÙˆÙƒÙ†Ø©
            train_dataset = train_dataset.map(self.tokenize_function, batched=True)
            eval_dataset = eval_dataset.map(self.tokenize_function, batched=True)
            
            # Ø¥Ø¹Ø¯Ø§Ø¯ Data Collator
            data_collator = DataCollatorForLanguageModeling(
                tokenizer=self.tokenizer,
                mlm=False
            )
            
            # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨
            training_args = TrainingArguments(
                output_dir=self.config["output_dir"],
                num_train_epochs=self.config["num_train_epochs"],
                per_device_train_batch_size=self.config["per_device_train_batch_size"],
                per_device_eval_batch_size=self.config["per_device_eval_batch_size"],
                learning_rate=self.config["learning_rate"],
                weight_decay=self.config["weight_decay"],
                logging_steps=self.config["logging_steps"],
                save_steps=self.config["save_steps"],
                eval_steps=self.config["eval_steps"],
                evaluation_strategy="steps",
                save_strategy="steps",
                warmup_steps=self.config["warmup_steps"],
                load_best_model_at_end=True,
                metric_for_best_model="eval_loss",
                greater_is_better=False,
                dataloader_pin_memory=False,
                report_to="none"  # ØªØ¹Ø·ÙŠÙ„ wandb
            )
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø¯Ø±Ø¨
            trainer = Trainer(
                model=model,
                args=training_args,
                train_dataset=train_dataset,
                eval_dataset=eval_dataset,
                data_collator=data_collator,
                tokenizer=self.tokenizer
            )
            
            # Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨
            logger.info("ğŸ”¥ Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¯Ø±ÙŠØ¨...")
            training_result = trainer.train()
            
            # Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            logger.info("ğŸ’¾ Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨...")
            trainer.save_model()
            self.tokenizer.save_pretrained(self.config["output_dir"])
            
            # Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù†ØªØ§Ø¦Ø¬
            self.generate_training_report(training_result)
            
            logger.info("ğŸ‰ ØªÙ… Ø§Ù†ØªÙ‡Ø§Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨Ù†Ø¬Ø§Ø­!")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ¯Ø±ÙŠØ¨: {e}")
            return False
    
    def generate_training_report(self, training_result):
        """Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ¯Ø±ÙŠØ¨"""
        report = {
            "training_completed": True,
            "training_loss": training_result.training_loss,
            "global_step": training_result.global_step,
            "config": self.config,
            "timestamp": datetime.now().isoformat()
        }
        
        with open("training_report.json", 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        logger.info("ğŸ“Š ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªØ¯Ø±ÙŠØ¨: training_report.json")

def main():
    """Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
    parser = argparse.ArgumentParser(description='Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…')
    parser.add_argument('--config', default='training_config.json', help='Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª')
    args = parser.parse_args()
    
    try:
        trainer = FineTuningTrainer(args.config)
        success = trainer.train_model()
        
        if success:
            print("âœ… ØªÙ… Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨Ù†Ø¬Ø§Ø­!")
            exit(0)
        else:
            print("âŒ ÙØ´Ù„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨!")
            exit(1)
            
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø£ Ø¹Ø§Ù…: {e}")
        exit(1)

if __name__ == "__main__":
    main()
''';

    final scriptFile = File('$_trainingDataPath/fine_tuning_trainer.py');
    await scriptFile.writeAsString(scriptContent);

    // Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù requirements.txt
    final requirementsContent = '''torch>=1.9.0
transformers>=4.20.0
datasets>=2.0.0
pandas>=1.3.0
numpy>=1.21.0
accelerate>=0.12.0
evaluate>=0.2.0
wandb>=0.12.0
scikit-learn>=1.0.0
''';

    final requirementsFile = File('$_trainingDataPath/requirements.txt');
    await requirementsFile.writeAsString(requirementsContent);

    print('[MODEL_TRAINING] âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø³ÙƒØ±ÙŠØ¨Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨');
  }

  // ØªØ­Ù…ÙŠÙ„ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨
  Future<void> _loadTrainingConfig() async {
    // ÙŠØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø¹Ù†Ø¯ Ø§Ù„Ø­Ø§Ø¬Ø© ÙÙŠ Ø³ÙƒØ±ÙŠØ¨Øª Python
    print('[MODEL_TRAINING] âœ… Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¬Ø§Ù‡Ø²Ø©');
  }

  // Ø­Ø§Ù„Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨
  bool get isTraining => _isTraining;
  double get trainingProgress => _trainingProgress;
  List<Map<String, dynamic>> get trainingLogs =>
      List.unmodifiable(_trainingLogs);

  // Ø¥Ù†Ø´Ø§Ø¡ Ø³ÙƒØ±ÙŠØ¨Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…Ø­Ø³Ù† Ù…Ø¹ Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø©
  Future<String> createTrainingScript(Map<String, dynamic> config) async {
    final scriptContent =
        '''#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…ØªÙ‚Ø¯Ù… Ù…Ø¹ Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø© ÙÙŠ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„ÙØ¹Ù„ÙŠ
Advanced Training System with Real-time Monitoring
"""

import os
import json
import pandas as pd
import numpy as np
import torch
import transformers
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling,
    EarlyStoppingCallback
)
from datasets import Dataset, load_dataset
import time
import sys
from datetime import datetime
import logging
from typing import Dict, List, Any

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('training_detailed.log', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

class AdvancedFineTuningTrainer:
    def __init__(self, config_path="training_config.json"):
        """ØªÙ‡ÙŠØ¦Ø© Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…"""
        self.config = self.load_config(config_path)
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.training_start_time = None
        self.progress_file = "training_progress.json"
        
        logger.info(f"ğŸš€ ØªÙ‡ÙŠØ¦Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¹Ù„Ù‰ {self.device}")
        logger.info(f"ğŸ’¾ Ø°Ø§ÙƒØ±Ø© GPU Ù…ØªØ§Ø­Ø©: {torch.cuda.get_device_properties(0).total_memory // 1024**3 if torch.cuda.is_available() else 'N/A'} GB")
        
    def load_config(self, config_path: str) -> Dict[str, Any]:
        """ØªØ­Ù…ÙŠÙ„ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…Ø­Ø³Ù†Ø©"""
        default_config = {
            "model_name": "${config['model_name']}",
            "dataset_path": "./fine_Tuning.json",
            "parquet_path": "./finetuning_examples.parquet",
            "output_dir": "./fine_tuned_model",
            "num_train_epochs": ${config['epochs']},
            "per_device_train_batch_size": ${config['batch_size']},
            "per_device_eval_batch_size": ${config['batch_size']},
            "learning_rate": ${config['learning_rate']},
            "weight_decay": 0.01,
            "logging_steps": ${config['logging_steps']},
            "save_steps": ${config['save_steps']},
            "eval_steps": ${config['save_steps']},
            "warmup_steps": ${config['warmup_steps']},
            "max_length": ${config['max_length']},
            "gradient_accumulation_steps": ${config['gradient_accumulation_steps']},
            "fp16": ${config['fp16']},
            "dataloader_num_workers": 4,
            "load_best_model_at_end": True,
            "metric_for_best_model": "eval_loss",
            "greater_is_better": False,
            "save_total_limit": 3
        }
        
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
            return {**default_config, **config}
        except FileNotFoundError:
            logger.warning("âš ï¸ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©")
            return default_config
    
    def prepare_massive_dataset(self) -> tuple:
        """ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¶Ø®Ù…Ø© Ù…Ù† multiple sources"""
        logger.info("ğŸ“Š ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¶Ø®Ù…Ø©...")
        self.update_progress(0.05, "ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø©")
        
        all_texts = []
        
        # 1. ØªØ­Ù…ÙŠÙ„ JSON data
        try:
            with open(self.config["dataset_path"], 'r', encoding='utf-8') as f:
                json_data = json.load(f)
            
            if 'cells' in json_data:
                for i, cell in enumerate(json_data['cells']):
                    if cell.get('cell_type') == 'code' and cell.get('source'):
                        source = cell['source']
                        if isinstance(source, list):
                            text = ''.join(source)
                        else:
                            text = str(source)
                        
                        if len(text.strip()) > 20:
                            all_texts.append(text.strip())
                    
                    # ØªØ­Ø¯ÙŠØ« Ø§Ù„ØªÙ‚Ø¯Ù… ÙƒÙ„ 1000 Ø¹Ù†ØµØ±
                    if i % 1000 == 0:
                        progress = 0.05 + (i / len(json_data['cells'])) * 0.15
                        self.update_progress(progress, f"Ù…Ø¹Ø§Ù„Ø¬Ø© JSON: {i}/{len(json_data['cells'])}")
                        
            logger.info(f"âœ… ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ {len(all_texts)} Ø¹ÙŠÙ†Ø© Ù…Ù† JSON")
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ JSON: {e}")
        
        # 2. ØªØ­Ù…ÙŠÙ„ Parquet data
        try:
            if os.path.exists(self.config["parquet_path"]):
                self.update_progress(0.20, "ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Parquet")
                df = pd.read_parquet(self.config["parquet_path"])
                
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†ØµÙˆØµ Ù…Ù† Ø§Ù„Ø¹Ù…ÙˆØ¯ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨
                text_columns = ['text', 'code', 'content', 'source']
                for col in text_columns:
                    if col in df.columns:
                        parquet_texts = df[col].dropna().astype(str).tolist()
                        valid_texts = [t for t in parquet_texts if len(t.strip()) > 20]
                        all_texts.extend(valid_texts)
                        logger.info(f"âœ… Ø£Ø¶ÙŠÙ {len(valid_texts)} Ø¹ÙŠÙ†Ø© Ù…Ù† Ø¹Ù…ÙˆØ¯ {col}")
                        break
                        
        except Exception as e:
            logger.error(f"âš ï¸ ØªØ¹Ø°Ø± ØªØ­Ù…ÙŠÙ„ Parquet: {e}")
        
        # 3. ÙÙ„ØªØ±Ø© ÙˆØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        self.update_progress(0.35, "ØªÙ†Ø¸ÙŠÙ ÙˆÙÙ„ØªØ±Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
        
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª
        all_texts = list(set(all_texts))
        
        # ÙÙ„ØªØ±Ø© Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù‚ØµÙŠØ±Ø© ÙˆØ§Ù„Ø·ÙˆÙŠÙ„Ø© Ø¬Ø¯Ø§Ù‹
        filtered_texts = []
        for text in all_texts:
            text_len = len(text)
            if 50 <= text_len <= 2048:  # Ù†ØµÙˆØµ Ù…ØªÙˆØ³Ø·Ø© Ø§Ù„Ø·ÙˆÙ„
                filtered_texts.append(text)
        
        logger.info(f"ğŸ“ˆ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©: {len(filtered_texts)} Ø¹ÙŠÙ†Ø© ØªØ¯Ø±ÙŠØ¨")
        logger.info(f"ğŸ“Š Ù…ØªÙˆØ³Ø· Ø·ÙˆÙ„ Ø§Ù„Ù†Øµ: {np.mean([len(t) for t in filtered_texts]):.1f} Ø­Ø±Ù")
        
        # Ø¥Ù†Ø´Ø§Ø¡ Dataset
        dataset = Dataset.from_dict({"text": filtered_texts})
        
        # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (90% ØªØ¯Ø±ÙŠØ¨ØŒ 10% ØªÙ‚ÙŠÙŠÙ…)
        split_dataset = dataset.train_test_split(test_size=0.1, seed=42)
        
        self.update_progress(0.40, f"ØªÙ… ØªØ­Ø¶ÙŠØ± {len(filtered_texts)} Ø¹ÙŠÙ†Ø© Ù„Ù„ØªØ¯Ø±ÙŠØ¨")
        
        return split_dataset["train"], split_dataset["test"]
    
    def tokenize_function(self, examples):
        """ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ tokens Ù…Ø­Ø³Ù†"""
        return self.tokenizer(
            examples["text"],
            truncation=True,
            padding="max_length",
            max_length=self.config["max_length"],
            return_attention_mask=True
        )
    
    def update_progress(self, progress: float, step: str):
        """ØªØ­Ø¯ÙŠØ« Ù…Ù„Ù Ø§Ù„ØªÙ‚Ø¯Ù… Ù„Ù„Ù…Ø±Ø§Ù‚Ø¨Ø©"""
        progress_data = {
            "progress": progress,
            "step": step,
            "timestamp": datetime.now().isoformat(),
            "elapsed_time": time.time() - self.training_start_time if self.training_start_time else 0
        }
        
        try:
            with open(self.progress_file, 'w', encoding='utf-8') as f:
                json.dump(progress_data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"ÙØ´Ù„ ÙÙŠ ØªØ­Ø¯ÙŠØ« Ø§Ù„ØªÙ‚Ø¯Ù…: {e}")
        
        logger.info(f"ğŸ”„ {step} - {progress*100:.1f}%")
    
    def train_model(self):
        """Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©"""
        try:
            self.training_start_time = time.time()
            logger.info("ğŸ”¥ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…...")
            
            # 1. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ù„ØªÙˆÙƒÙ†Ø§ÙŠØ±
            self.update_progress(0.00, "ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ù„Ù…Ø­Ù„Ù„ Ø§Ù„Ù„ØºÙˆÙŠ")
            
            self.tokenizer = AutoTokenizer.from_pretrained(self.config["model_name"])
            
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            model = AutoModelForCausalLM.from_pretrained(
                self.config["model_name"],
                torch_dtype=torch.float16 if self.config["fp16"] else torch.float32,
                device_map="auto" if torch.cuda.is_available() else None
            )
            
            # 2. ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¶Ø®Ù…Ø©
            train_dataset, eval_dataset = self.prepare_massive_dataset()
            
            # 3. ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªÙˆÙƒÙ†Ø©
            self.update_progress(0.45, "ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªÙˆÙƒÙ†Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
            
            train_dataset = train_dataset.map(
                self.tokenize_function, 
                batched=True,
                remove_columns=train_dataset.column_names,
                desc="Tokenizing train dataset"
            )
            
            eval_dataset = eval_dataset.map(
                self.tokenize_function, 
                batched=True,
                remove_columns=eval_dataset.column_names,
                desc="Tokenizing eval dataset"
            )
            
            # 4. Ø¥Ø¹Ø¯Ø§Ø¯ Data Collator
            data_collator = DataCollatorForLanguageModeling(
                tokenizer=self.tokenizer,
                mlm=False
            )
            
            # 5. Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…Ø­Ø³Ù†Ø©
            self.update_progress(0.50, "Ø¥Ø¹Ø¯Ø§Ø¯ Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨")
            
            training_args = TrainingArguments(
                output_dir=self.config["output_dir"],
                num_train_epochs=self.config["num_train_epochs"],
                per_device_train_batch_size=self.config["per_device_train_batch_size"],
                per_device_eval_batch_size=self.config["per_device_eval_batch_size"],
                learning_rate=self.config["learning_rate"],
                weight_decay=self.config["weight_decay"],
                logging_steps=self.config["logging_steps"],
                save_steps=self.config["save_steps"],
                eval_steps=self.config["eval_steps"],
                evaluation_strategy="steps",
                save_strategy="steps",
                warmup_steps=self.config["warmup_steps"],
                gradient_accumulation_steps=self.config["gradient_accumulation_steps"],
                fp16=self.config["fp16"],
                dataloader_num_workers=self.config["dataloader_num_workers"],
                load_best_model_at_end=self.config["load_best_model_at_end"],
                metric_for_best_model=self.config["metric_for_best_model"],
                greater_is_better=self.config["greater_is_better"],
                save_total_limit=self.config["save_total_limit"],
                report_to="none",
                logging_dir="./logs",
                run_name=f"fine_tuning_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            )
            
            # 6. Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù…Ø¹ Early Stopping
            trainer = Trainer(
                model=model,
                args=training_args,
                train_dataset=train_dataset,
                eval_dataset=eval_dataset,
                data_collator=data_collator,
                tokenizer=self.tokenizer,
                callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]
            )
            
            # 7. Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„ÙØ¹Ù„ÙŠ
            self.update_progress(0.55, "Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„ÙØ¹Ù„ÙŠ Ù„Ù„Ù†Ù…ÙˆØ°Ø¬")
            logger.info("âš¡ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„ÙØ¹Ù„ÙŠ...")
            
            training_result = trainer.train()
            
            # 8. Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            self.update_progress(0.95, "Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨")
            logger.info("ğŸ’¾ Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬...")
            
            trainer.save_model()
            self.tokenizer.save_pretrained(self.config["output_dir"])
            
            # 9. Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± Ù…ÙØµÙ„
            self.generate_detailed_report(training_result, train_dataset, eval_dataset)
            
            self.update_progress(1.0, "ØªÙ… Ø§Ù†ØªÙ‡Ø§Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨Ù†Ø¬Ø§Ø­")
            logger.info("ğŸ‰ ØªÙ… Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨Ù†Ø¬Ø§Ø­!")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ¯Ø±ÙŠØ¨: {e}")
            self.update_progress(0.0, f"Ø®Ø·Ø£: {str(e)}")
            return False
    
    def generate_detailed_report(self, training_result, train_dataset, eval_dataset):
        """Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± Ù…ÙØµÙ„ Ù„Ù„Ù†ØªØ§Ø¦Ø¬"""
        total_time = time.time() - self.training_start_time
        
        report = {
            "training_completed": True,
            "model_name": self.config["model_name"],
            "training_loss": training_result.training_loss,
            "global_step": training_result.global_step,
            "total_training_time_minutes": total_time / 60,
            "train_dataset_size": len(train_dataset),
            "eval_dataset_size": len(eval_dataset),
            "epochs_completed": self.config["num_train_epochs"],
            "config": self.config,
            "device_used": str(self.device),
            "timestamp": datetime.now().isoformat(),
            "performance_metrics": {
                "samples_per_second": len(train_dataset) / total_time,
                "total_parameters": sum(p.numel() for p in model.parameters() if p.requires_grad)
            }
        }
        
        # Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ±
        report_files = [
            "training_report.json",
            "detailed_training_report.json"
        ]
        
        for report_file in report_files:
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
        
        logger.info("ğŸ“Š ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù…ÙØµÙ„")

def main():
    """Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ø§Ù„Ù…Ø­Ø³Ù†Ø©"""
    try:
        trainer = AdvancedFineTuningTrainer()
        success = trainer.train_model()
        
        if success:
            print("âœ… ØªÙ… Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨Ù†Ø¬Ø§Ø­!")
            exit(0)
        else:
            print("âŒ ÙØ´Ù„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨!")
            exit(1)
            
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø£ Ø¹Ø§Ù…: {e}")
        exit(1)

if __name__ == "__main__":
    main()
''';

    final scriptFile = File(
      '$_trainingDataPath/advanced_fine_tuning_trainer.py',
    );
    await scriptFile.writeAsString(scriptContent);

    print('[MODEL_TRAINING] âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø³ÙƒØ±ÙŠØ¨Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…');
    return scriptFile.path;
  }

  // Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…Ø­Ø³Ù† Ù…Ø¹ Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©
  Future<bool> startTrainingAdvanced({
    required String scriptPath,
    required Function(double, String) onProgress,
    required Function(String) onLog,
  }) async {
    try {
      onLog('ğŸš€ Ø¨Ø¯Ø¡ ØªØ´ØºÙŠÙ„ Ø³ÙƒØ±ÙŠØ¨Øª Python Ø§Ù„Ù…ØªÙ‚Ø¯Ù…...');

      // Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ù…ØªØ·Ù„Ø¨Ø§Øª Python
      final pythonCheck = await _checkPythonRequirements();
      if (!pythonCheck) {
        onLog('âŒ Ù…ØªØ·Ù„Ø¨Ø§Øª Python ØºÙŠØ± Ù…ÙƒØªÙ…Ù„Ø©');
        return false;
      }

      // ØªØ´ØºÙŠÙ„ Ø³ÙƒØ±ÙŠÙ¾Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨
      final process = await Process.start('python3', [
        'advanced_fine_tuning_trainer.py',
      ], workingDirectory: _trainingDataPath);

      // Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„ØªÙ‚Ø¯Ù… ÙˆØ§Ù„Ø¥Ø®Ø±Ø§Ø¬
      _monitorAdvancedTraining(process, onProgress, onLog);

      final exitCode = await process.exitCode;

      if (exitCode == 0) {
        onLog('ğŸ‰ ØªÙ… Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨Ù†Ø¬Ø§Ø­!');
        return true;
      } else {
        onLog('âŒ ÙØ´Ù„ ÙÙŠ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ - Exit Code: $exitCode');
        return false;
      }
    } catch (e) {
      onLog('âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨: $e');
      return false;
    }
  }

  // ÙØ­Øµ Ù…ØªØ·Ù„Ø¨Ø§Øª Python
  Future<bool> _checkPythonRequirements() async {
    try {
      // ÙØ­Øµ Python
      final pythonResult = await Process.run('python3', ['--version']);
      if (pythonResult.exitCode != 0) {
        print('[REQUIREMENTS] âŒ Python ØºÙŠØ± Ù…ØªØ§Ø­');
        return false;
      }

      // ÙØ­Øµ pip
      final pipResult = await Process.run('python3', [
        '-m',
        'pip',
        '--version',
      ]);
      if (pipResult.exitCode != 0) {
        print('[REQUIREMENTS] âŒ pip ØºÙŠØ± Ù…ØªØ§Ø­');
        return false;
      }

      // ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª
      print('[REQUIREMENTS] ğŸ“¦ ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª...');
      final installResult = await Process.run('python3', [
        '-m',
        'pip',
        'install',
        '-r',
        'requirements.txt',
      ], workingDirectory: _trainingDataPath);

      if (installResult.exitCode == 0) {
        print('[REQUIREMENTS] âœ… ØªÙ… ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø¨Ù†Ø¬Ø§Ø­');
        return true;
      } else {
        print('[REQUIREMENTS] âš ï¸ ØªØ­Ø°ÙŠØ± ÙÙŠ ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª');
        return true; // Ù†ÙƒÙ…Ù„ Ø­ØªÙ‰ Ù„Ùˆ ÙÙŠÙ‡ ØªØ­Ø°ÙŠØ±Ø§Øª
      }
    } catch (e) {
      print('[REQUIREMENTS] âŒ Ø®Ø·Ø£ ÙÙŠ ÙØ­Øµ Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª: $e');
      return false;
    }
  }

  // Ù…Ø±Ø§Ù‚Ø¨Ø© Ù…ØªÙ‚Ø¯Ù…Ø© Ù„Ù„ØªØ¯Ø±ÙŠØ¨
  void _monitorAdvancedTraining(
    Process process,
    Function(double, String) onProgress,
    Function(String) onLog,
  ) {
    // Ù…Ø±Ø§Ù‚Ø¨Ø© Ù…Ù„Ù Ø§Ù„ØªÙ‚Ø¯Ù…
    Timer.periodic(const Duration(seconds: 2), (timer) {
      _checkProgressFile(onProgress, timer);
    });

    // Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬
    process.stdout.transform(utf8.decoder).listen((data) {
      final lines = data.split('\n');
      for (final line in lines) {
        if (line.trim().isNotEmpty) {
          onLog('[TRAINING] $line');
          _parseTrainingOutput(line, onProgress);
        }
      }
    });

    process.stderr.transform(utf8.decoder).listen((data) {
      final lines = data.split('\n');
      for (final line in lines) {
        if (line.trim().isNotEmpty) {
          onLog('[ERROR] $line');
        }
      }
    });
  }

  // ÙØ­Øµ Ù…Ù„Ù Ø§Ù„ØªÙ‚Ø¯Ù…
  void _checkProgressFile(Function(double, String) onProgress, Timer timer) {
    try {
      final progressFile = File('$_trainingDataPath/training_progress.json');
      if (progressFile.existsSync()) {
        final content = progressFile.readAsStringSync();
        final progress = jsonDecode(content);

        onProgress(
          progress['progress']?.toDouble() ?? 0.0,
          progress['step'] ?? 'Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªØ¯Ø±ÙŠØ¨...',
        );

        // Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ù…Ø¤Ù‚Øª Ø¹Ù†Ø¯ Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡
        if ((progress['progress']?.toDouble() ?? 0.0) >= 1.0) {
          timer.cancel();
        }
      }
    } catch (e) {
      // ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© Ù…Ù„Ù Ø§Ù„ØªÙ‚Ø¯Ù…
    }
  }

  // ØªØ­Ù„ÙŠÙ„ Ø¥Ø®Ø±Ø§Ø¬ Ø§Ù„ØªØ¯Ø±ÙŠØ¨
  void _parseTrainingOutput(String line, Function(double, String) onProgress) {
    // Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªÙ‚Ø¯Ù… Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù…Ù† Ø¥Ø®Ø±Ø§Ø¬ transformers
    if (line.contains('Training:')) {
      final regex = RegExp(r'(\d+)%');
      final match = regex.firstMatch(line);
      if (match != null) {
        final percentage = double.tryParse(match.group(1)!) ?? 0.0;
        onProgress(0.5 + (percentage / 100.0) * 0.45, 'ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬...');
      }
    }
  }

  // Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…Ø­Ø³Ù†
  Future<void> stopTrainingAdvanced() async {
    // Ù‚ØªÙ„ Ø¹Ù…Ù„ÙŠØ§Øª Python
    try {
      await Process.run('pkill', ['-f', 'advanced_fine_tuning_trainer.py']);
      print('[MODEL_TRAINING] â¹ï¸ ØªÙ… Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„ØªØ¯Ø±ÙŠØ¨');
    } catch (e) {
      print('[MODEL_TRAINING] âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¹Ù…Ù„ÙŠØ© Ù„Ù„Ø¥ÙŠÙ‚Ø§Ù');
    }
  }

  // Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
  Future<Map<String, dynamic>> getDatasetInfo() async {
    try {
      final Map<String, dynamic> info = {};

      // Ù…Ø¹Ù„ÙˆÙ…Ø§Øª JSON
      final jsonFile = File('$_trainingDataPath/fine_Tuning.json');
      if (await jsonFile.exists()) {
        final content = await jsonFile.readAsString();
        final data = jsonDecode(content);

        if (data['cells'] != null) {
          final codeCells = (data['cells'] as List)
              .where((cell) => cell['cell_type'] == 'code')
              .length;

          info['json_cells'] = data['cells'].length;
          info['json_code_cells'] = codeCells;
        }
      }

      // Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Parquet
      final parquetFile = File(
        '$_trainingDataPath/finetuning_examples.parquet',
      );
      if (await parquetFile.exists()) {
        final size = await parquetFile.length();
        info['parquet_size_mb'] = size / (1024 * 1024);
      }

      return info;
    } catch (e) {
      print('[DATASET_INFO] âŒ Ø®Ø·Ø£: $e');
      return {};
    }
  }

  // ØªØµØ¯ÙŠØ± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
  Future<String?> exportModel() async {
    try {
      final modelDir = Directory('$_trainingDataPath/fine_tuned_model');
      if (!await modelDir.exists()) {
        return null;
      }

      final exportDir = Directory('$_trainingDataPath/exported_model');
      if (!await exportDir.exists()) {
        await exportDir.create(recursive: true);
      }

      // Ù†Ø³Ø® Ù…Ù„ÙØ§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
      await for (final file in modelDir.list()) {
        if (file is File) {
          final fileName = file.path.split('/').last;
          final targetFile = File('${exportDir.path}/$fileName');
          await file.copy(targetFile.path);
        }
      }

      return exportDir.path;
    } catch (e) {
      print('[EXPORT] âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØµØ¯ÙŠØ±: $e');
      return null;
    }
  }

  // ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
  Future<Map<String, dynamic>?> evaluateModel() async {
    try {
      final reportFile = File(
        '$_trainingDataPath/detailed_training_report.json',
      );
      if (await reportFile.exists()) {
        final content = await reportFile.readAsString();
        return jsonDecode(content);
      }
      return null;
    } catch (e) {
      print('[EVALUATION] âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…: $e');
      return null;
    }
  }

  // ÙˆØ§Ø¬Ù‡Ø§Øª Ø¨Ø³ÙŠØ·Ø© Ù„Ù„ØªØ¯Ø±ÙŠØ¨ (ØªØ­ÙˆÙŠÙ„ Ù„Ù„Ø¯ÙˆØ§Ù„ Ø§Ù„Ù…Ø­Ø³Ù†Ø©)
  Future<bool> startTraining({
    required String scriptPath,
    required Function(double, String) onProgress,
    required Function(String) onLog,
  }) => startTrainingAdvanced(
    scriptPath: scriptPath,
    onProgress: onProgress,
    onLog: onLog,
  );

  Future<void> stopTraining() => stopTrainingAdvanced();

  // ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯
  void dispose() {
    // ØªÙ†Ø¸ÙŠÙ Ø£ÙŠ Ù…ÙˆØ§Ø±Ø¯ Ù…ÙØªÙˆØ­Ø©
  }

  // ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨
  Future<ModelAnalysis> analyzeTrainedModel(String modelPath) async {
    try {
      // Ù‚Ø±Ø§Ø¡Ø© Ù…Ù„ÙØ§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
      final modelDir = Directory(modelPath);
      if (!await modelDir.exists()) {
        throw Exception('Ù…Ø³Ø§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯');
      }

      // ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª
      final files = await modelDir.list().toList();
      final modelSize = await _calculateDirectorySize(modelDir);

      // Ù‚Ø±Ø§Ø¡Ø© Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
      final configFile = File('$modelPath/config.json');
      Map<String, dynamic>? modelConfig;
      if (await configFile.exists()) {
        final configContent = await configFile.readAsString();
        modelConfig = jsonDecode(configContent);
      }

      return ModelAnalysis(
        modelPath: modelPath,
        sizeInMB: modelSize / (1024 * 1024),
        fileCount: files.length,
        config: modelConfig,
        createdAt: DateTime.now(),
      );
    } catch (e) {
      throw Exception('ÙØ´Ù„ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: $e');
    }
  }

  // Ø­Ø³Ø§Ø¨ Ø­Ø¬Ù… Ø§Ù„Ù…Ø¬Ù„Ø¯
  Future<int> _calculateDirectorySize(Directory directory) async {
    int size = 0;
    await for (final entity in directory.list(recursive: true)) {
      if (entity is File) {
        size += await entity.length();
      }
    }
    return size;
  }
}

// Ù†Ù…ÙˆØ°Ø¬ Ù†ØªÙŠØ¬Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨
class TrainingResult {
  final bool success;
  final double finalLoss;
  final int epochs;
  final String modelPath;
  final List<Map<String, dynamic>> logs;
  final String? error;

  TrainingResult({
    required this.success,
    required this.finalLoss,
    required this.epochs,
    required this.modelPath,
    required this.logs,
    this.error,
  });
}

// Ù†Ù…ÙˆØ°Ø¬ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
class ModelAnalysis {
  final String modelPath;
  final double sizeInMB;
  final int fileCount;
  final Map<String, dynamic>? config;
  final DateTime createdAt;

  ModelAnalysis({
    required this.modelPath,
    required this.sizeInMB,
    required this.fileCount,
    this.config,
    required this.createdAt,
  });
}
